# AI &amp; ML Project 2023
### Dataset : ShopEasy

### Members: Kevin Wang (289121), Ahmet Akgun(287241), Kaixin Lai(289991)

## [Section 1] Introduction
In this project we are assigned to discover and analyze buying habits and behaviors of customers in a leading e-commerse site called ShopEasy. The dataset contains a wide variety of features that can be analyzed and compared with eachother. Through this analysis we are able to distinguish differences between redundant and essential features to then aid us in the categorization of the customer base. We aim to elevate the shopping experience for every customer, creating a more personal enviorment customized to their unique preferences and needs. This is done by understanding the different clusters that we achieve, observing prominent features in them and creating a targetted experience for each cluster.

## [Section 2] Method
In our project we firstly wanted to visualize the data and each of its features. We started with an EDA(exploratory data analysis) and used various techniques to observe each features and their relationship between eachother. From the description of the dataframe we can already notice some data that doesn't match. The most obvious example is in how the max value of leastAmountPaid is higher than the max value of account total. This could be due to a recording error, or something similar which will further be analyzed later on. We checked for data integrity by observing whether there were duplicates or missing values. There were no duplicates but a small percentage of missing values(around 3.5%) in the columns 'leastAmountPaid' and 'maxSpendLimit' where we chose to drop the rows containing the missing values as the percentage was so small. 
 
 Next we performed univariate and bivariate analysis on the data. Firstly we did an univariate analysis, which provides insight on how a single variable is distributed across the dataframe. The analysis is conducted using boxplot and histogram to visualize the data as most of the data are numerical features. From this data we noticed a lot of the features have right skewed distributions such as accountTotal, itemCosts, maxSpendLimit ect. A few have left skewed distributions such as frequencyIndex and accountLifespan. itemBuyFrequency instead has a bimodal distribution and webusage has a uniform distirbution. Skewed distribution can suggest a high concentration of values on the higher or lower end of the scale, depending on left or right skewed distribution respectively and can suggest that outliers are present. Therefore we created boxplots to visualize the outliers. In numerous of the box plots we observe a large number of outliers that suggest there is high variations in some of these features. However, before further analyzing the outliers we will want to do that after choosing the features that fit best for the purpose of clustering. 
 Below is an image of the first nine histograms and boxplots:
<img width="1181" alt="Screenshot 2024-01-02 at 01 07 15" src="https://github.com/kevinwang2307/ShopEasy-289121/assets/145768116/86d77641-c6bc-481d-a41d-7db7c3b3b0ce">
<img width="1181" alt="Screenshot 2024-01-02 at 01 07 32" src="https://github.com/kevinwang2307/ShopEasy-289121/assets/145768116/2f0a0c78-aa08-413e-a19a-34b07a749f8d">
 
 We proceeded with bivariate analysis to examine the relationships between pairs of variables. Here we choosed to examine pairs of variables that showed similar values to eachother from the boxplots. We noticed similar values between itemCosts and singleItemCosts as well as itemBuyFrequency and multipleItemBuyFrequency. Anything that is missed will be further analyzed in the multivariate analysis. Another interesting observation that could be analyzed is how leastAmountPaid has outliers that reach around 80000 in value, which is much higher than the highest value in monthlyPaid or worst accountTotal. This also raises some question on the integrity of the attribute monthlyPaid as the description of the feature is very vague only describing "Total amount paid by the user every month" which is assuming an user pays the same total amount every month or it might be an average. Instead for accountTotal, not only does leastAmountPaid have higher values than it, which would already be an anomaly as in no case would "The least amount paid by the user in a single transaction" be higher than the "Total amount spent by the user on ShopEasy since their registration". This is because leastAmountPaid should be included in the total amount spent by user on ShopEasy, so under normal circumstances accountTotal should always be higher than leastAmountPaid. 
 
 We then further observed the distribution of categorical features accountType and location and discovered that both categorical features have equal distribution across all users. We also used KDE plots to understand whether there is a difference in distribution between the categorical feature distributions to another feature such as itemBuyFrequency and discovered that location feature might not be relevant to understanding buying habits and behaviour of customers, as they have no effect on the distribution of other features and in a e-commerce context where almost everything is done online, on a website, location does not really matter. Although accountType did not have an affect on feature distribution as well, we wanted to keep accountType as it is a feature that would provide more future insight on the user base of each cluster.
 
 Before we used a heatmap to visualize all the correlations we label encoded from sklearn to encode accountType categorical values. We use label encoding for accountType as they can be considered ordinal(where the order of categories is meaningful'premium' being first and 'student' being last). The reasoning as premium account type pay the most, while student account type get discounts so they pay the least. And regular account types pay a normal amount. For location categorical values, since it is a nominal data, we chose not to use label encoding as it might assume a natural order in the location, instead using dummy variables would be more appropriate in this case where each category is represented by a binary vector. We then observed the correlation heatmap to observe the features in a more holistic way. Below are the notes we took observing the heatmap:
  
  Firstly, we can confirm that there is no correlation between the locations and other attributes. As mentioned before since we are analyzing customers on an e-commerce website, the location often does not matter unless it in a whole different country, which in this case it is not. Therefore we decided not to select this feature as it is not relevant to our analysis. It is different for account type as this categorical feature is actually relevant to our analysis even though it has no coorrelation seen in the heatmap with any other feature. From the heatmap one can also observe a few features that can be considered irrelevant for the analysis or redundant. We choose to drop accountTotal as although it can be an essential feature to evaluate a user's engagement with the e-commerce website, the values it portrayed were inconsistent compared to other attributes such as itemCosts, and monthlypaid meaning either the total amount does not depict a value in money or uses a different currency or in general is inaccurate. Using this logic we also choose to drop leastAmountPaid, having little to no correlation with the other features, as mentioned before, the values portrayed are inconsistent compared to other attributes such as monthlyPaid. After further observation we notice that the attribute ,monthlyPaid also has inconsistencies with attributes such as itemCosts and also add no additional insight that itemCosts wouldn't add therefore we choose to drop monthlyPaid. This also leads to dropping features such as webUsage and frequencyIndex, not only having little to no correlation with the rest of the features but both also have a very similar definition to itemBuyFrequency which actually does have correlation to other features that are relevant to our analysis. We also drop the feature paymentCompletionRate for the lack of correlation with other features as well as the lack of insight it can provide. We also consider dropping maxSpendLimit for the same reason as accountTotal and leastAmountPaid, which is for the reason that it proves to have inconsistencies between other features such as monthly paid, where although the limit is set to a certain value for a user, that same user would have higher singleItemCosts value than the maxSpendLimit value. "The maximum amount the user can spend in a single purchase, set by ShopEasy based on user's buying behavior and loyalty" is the definition of maxSpendLimit and the definition for singleItemCosts is "Costs of items that the user bought in a single purchase without opting for installments" so therefore it is again an inconsistent value and we decide to drop maxSpendLimit. Additonally features such as singleItemBuyFrequency and multipleItemBuyFrequency or singleItemCosts and multipleItemCosts are a more specific version of itemBuyFrequency and itemCosts respectively. These two pair of features can be generalized by itemBuyFrequency and itemCosts and would provide a more significant insight to our analysis compared to the general insigh provided by itemBuyFrequency and itemCosts. Therefore itemBuyFrequency and itemCosts are dropped. We also drop emergencyUseFrequency due to its high correlation and similar definition to emergencyCount, meaning it could be considered redundant to have it.
Below is an image of the heat map: 
<img width="1029" alt="Screenshot 2024-01-02 at 01 08 20" src="https://github.com/kevinwang2307/ShopEasy-289121/assets/145768116/e4e2ac54-dd63-4a6d-bab7-ea48470615b3">

 
  Now that we have observed and analyzed the data we moved on to preprocessing our data. Since we dropped the columns leastAmountPaid and maxSpendLimit which were the columns who originally had the missing data we added the missing values back as they wouldn't be missing for the new columns that are present in the dataframe. We dropped the features we deemed redudant or not insightful and removed outliers that were 4 standard deviations away from the mean of the remaining features. Since the range of the feature values differs from feature to feature we also scaled the data.
 
  Moving on we chose what type of problem this is and we understood it was a clustering problem. This is because the dataset provides a mix of numerical and categorical data, ideal for segmentation and clustering analysis. From this we could already rule out regression as it is used to predict a continuous outcome. Instead this question focuses on predicted clusters of customers for a more personal user experience. Classification is not chosen as well because it is used in supervised learning meaning, is used when there is already a predefined categorical outcome such as whether the clothes will be on sale or not. In this case, although we will be dividing the customers/users into clusters(categories) it is not the same as in the sense that the clusters are not predefined but rather require to be uncovered through analysis of the different clusters that are formed when applying different clustering algorithms. In fact clustering problem is used in unsupervised learning where there is no predefined outcome.
  
  We chose to test 3 models of clustering, K-means++, hierchical clustering and DBSCAN. There are different reason for which we chose each one. We chose to do all 3 for a more comparative analysis where each method could provide insight on clustering the same data, it can reveal more aspects of the clusters. Firstly, K-means which is used to deal with larger datasets generally works better than hierchical clustering. It is also very versatile for a wide range of applications. Hierchical clustering instead uses a dendrogram that makes the clustering process more interpretable through a visual aspect. DBSCAN is the only model out of the 3 that does not need a predefined cluster number, instead it needs an eps value and min_samples values( will be explained in the next section) and is especially good at handling noise as it removes outliers completely from the clustering process and considers them a seperate clusters. Both hierchical clustering and K-means clustering are defined as linear clustering models as they are able to effectively handle linearly separable data. On the other hand, we have DBSCAN which is reffered to as a non linear clustering algorithm as it identifies clusters with more complex shapes that are not linearly separable. By linearly seperable I mean that clusters can be divided by a straight line on a hyper plane. We decided that for our analysis of the clusters we would only use hierchical clustering and K-means clustering as later on we discover that some outliers are needed to create meaningful and insightful clusters that we can analyze which DBSCAN does not allow us to do. Below a flowchart illustrating the steps of the project is depicted:
<img width="1049" alt="Screenshot 2024-01-01 at 03 15 39" src="https://github.com/kevinwang2307/ShopEasy-289121/assets/145768116/4deaad80-e7be-4536-b319-12852e3fb3c6">

We also made a conda environment called shopEasy_environment.yml to ensure accsibility and functionality of the jupyter notebook with all the needed imports.

To make the environment work just download the shopEasy_environment.yml in the repository, open a terminal and run "conda env create -f shopEasy_environment.yml" and then "conda activate shopEasy_env". Finally run "jupyter notebook".(don't include the quotation marks in your code)


